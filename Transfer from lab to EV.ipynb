{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed08f2-2314-40c4-ac0f-a4d7318cdf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import CubicSpline\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rcParams\n",
    "\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "PAST = 100\n",
    "FUTURE = 10\n",
    "TOTAL_WINDOW = PAST + FUTURE\n",
    "REAL_SEQ_DIM = 28\n",
    "\n",
    "def load_pretrained_from_lab(model: Visual_Module_Real,\n",
    "                             pretrained_path: str):\n",
    "    if not os.path.exists(pretrained_path):\n",
    "        print(f\"Warning: Pretrained model not found: {pretrained_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoading pretrained weights from lab model: {pretrained_path}\")\n",
    "    state_dict = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k in model_dict and model_dict[k].shape == v.shape:\n",
    "            compatible_dict[k] = v\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print(f\"  - Compatible parameters: {len(compatible_dict)}/{len(model_dict)}\")\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"  ✅ Pretrained weights loaded (incompatible layers skipped)\")\n",
    "\n",
    "def freeze_backbone_for_transfer(model: Visual_Module_Real):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    train_keys = [\"temporal_module\", \"branch_att\", \"decoder_4\", \"output_layer\"]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(k in name for k in train_keys):\n",
    "            param.requires_grad = True\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTransfer learning setup:\")\n",
    "    print(f\"  - Total parameters: {total_params / 1e6:.2f} M\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params / 1e6:.2f} M\")\n",
    "    print(f\"  - Training modules: {train_keys}\")\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image, sequence, targets):\n",
    "        self.image = torch.tensor(image, dtype=torch.float32)\n",
    "        self.sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.image[idx],\n",
    "                self.sequence[idx],\n",
    "                self.targets[idx])\n",
    "\n",
    "def train_model(train_loader, val_loader, model, epochs,\n",
    "                criterion, optimizer, scheduler, save_path):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"\\nTraining configuration (real vehicle transfer):\")\n",
    "    print(f\"  - Device: {device}\")\n",
    "    print(f\"  - Training set size: {len(train_loader.dataset)}\")\n",
    "    print(f\"  - Validation set size: {len(val_loader.dataset)}\")\n",
    "    print(f\"  - Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"  - Total epochs: {epochs}\")\n",
    "    print(f\"  - Model save path: {save_path}\\n\")\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    total_train_time = 0.0\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        batch_loss = []\n",
    "        train_loader_tqdm = tqdm(train_loader,\n",
    "                                 desc=f\"[Real vehicle transfer] Training Epoch {epoch + 1}/{epochs}\")\n",
    "        for img_in, seq_in, targets in train_loader_tqdm:\n",
    "            img_in = img_in.to(device)\n",
    "            seq_in = seq_in.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(img_in, seq_in)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item())\n",
    "            train_loader_tqdm.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "\n",
    "        train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "        model.eval()\n",
    "        val_batch_loss = []\n",
    "        with torch.no_grad():\n",
    "            for img_in, seq_in, targets in val_loader:\n",
    "                img_in = img_in.to(device)\n",
    "                seq_in = seq_in.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(img_in, seq_in)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_batch_loss.append(loss.item())\n",
    "\n",
    "        val_loss.append(np.mean(val_batch_loss))\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        total_train_time += epoch_time\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss[-1])\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            current_lr = None\n",
    "\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1]\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            lr_str = f\", LR: {current_lr:.2e}\" if current_lr is not None else \"\"\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss[-1]:.6f}, \"\n",
    "                f\"Val Loss: {val_loss[-1]:.6f}{lr_str}, Time: {epoch_time:.2f}s ⭐ (saved best model)\"\n",
    "            )\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            lr_str = f\", LR: {current_lr:.2e}\" if current_lr is not None else \"\"\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss[-1]:.6f}, \"\n",
    "                f\"Val Loss: {val_loss[-1]:.6f}{lr_str}, Time: {epoch_time:.2f}s\"\n",
    "            )\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered: validation loss not improved for {patience} epochs\")\n",
    "            break\n",
    "\n",
    "    avg_epoch_time = np.mean(epoch_times)\n",
    "    total_train_time_min = total_train_time / 60\n",
    "    \n",
    "    print(\"\\nReal vehicle transfer training completed!\")\n",
    "    print(f\"  - Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"  - Total training time: {total_train_time_min:.2f} minutes\")\n",
    "    print(f\"  - Average epoch time: {avg_epoch_time:.2f} seconds\")\n",
    "\n",
    "    return train_loss, val_loss, total_train_time_min, avg_epoch_time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
