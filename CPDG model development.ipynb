{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d71d44-4d15-44da-869f-f5c913df4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import CubicSpline\n",
    "import time  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rcParams\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "PAST = 100\n",
    "FUTURE = 10\n",
    "TOTAL_WINDOW = PAST + FUTURE\n",
    "SOH_MIN_EVAL = 0.8\n",
    "SOH_MAX_EVAL = 1.0\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x).view(x.size(0), -1))\n",
    "        max_out = self.fc(self.max_pool(x).view(x.size(0), -1))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        \n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        x_att = self.conv(x_cat)\n",
    "        return self.sigmoid(x_att)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, max(1, channels // reduction)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(max(1, channels // reduction), channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class Combined_Temporal_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Combined_Temporal_Module, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=15, \n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            bidirectional=True  \n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 512, kernel_size=4, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv1_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(512)\n",
    "        self.conv1_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(512)\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=4)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv2_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(512)\n",
    "        self.conv2_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(512)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.upconv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.upconv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Visual_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Visual_Module, self).__init__()\n",
    "        self.temporal_module = Combined_Temporal_Module()\n",
    "        self.se_img = SEBlock(512)\n",
    "        self.se_seq = SEBlock(512)\n",
    "        self.cbam = CBAM(1024, reduction_ratio=16, kernel_size=7)\n",
    "        self.branch_att = nn.Sequential(\n",
    "            nn.Linear(512 + 512, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        self.encoder_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.encoder_2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.encoder_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.encoder_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.encoder_5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 256, kernel_size=2, stride=2)\n",
    "        self.bn_up1 = nn.BatchNorm2d(256)\n",
    "        self.decoder_1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.bn_up2 = nn.BatchNorm2d(128)\n",
    "        self.decoder_2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.bn_up3 = nn.BatchNorm2d(64)\n",
    "        self.decoder_3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upconv4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.bn_up4 = nn.BatchNorm2d(32)\n",
    "        self.decoder_4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, s, return_att=False, branch_gates=None, return_stages=False):\n",
    "        stages = {} if return_stages else None\n",
    "        x_input = x\n",
    "        if return_stages:\n",
    "            stages[\"input\"] = x_input\n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = self.encoder_1(x)\n",
    "        if return_stages:\n",
    "            stages[\"enc1\"] = x1\n",
    "        x2_in = self.pool(x1)\n",
    "        x2 = self.encoder_2(x2_in)\n",
    "        if return_stages:\n",
    "            stages[\"enc2\"] = x2\n",
    "        x3_in = self.pool(x2)\n",
    "        x3 = self.encoder_3(x3_in)\n",
    "        if return_stages:\n",
    "            stages[\"enc3\"] = x3\n",
    "        x4_in = self.pool(x3)\n",
    "        x4 = self.encoder_4(x4_in)\n",
    "        if return_stages:\n",
    "            stages[\"enc4\"] = x4\n",
    "        x5_in = self.pool(x4)\n",
    "        x5 = self.encoder_5(x5_in)\n",
    "        if return_stages:\n",
    "            stages[\"enc5\"] = x5\n",
    "        s = self.temporal_module(s)\n",
    "        if return_stages:\n",
    "            stages[\"seq_branch\"] = s\n",
    "        x5 = self.se_img(x5)\n",
    "        s = self.se_seq(s)\n",
    "        gap = nn.functional.adaptive_avg_pool2d\n",
    "        g_img = torch.flatten(gap(x5, 1), 1)\n",
    "        g_seq = torch.flatten(gap(s, 1), 1)\n",
    "        att_logits = self.branch_att(torch.cat([g_img, g_seq], dim=1))\n",
    "        att_weights = torch.softmax(att_logits, dim=1)\n",
    "        if branch_gates is not None:\n",
    "            if not torch.is_tensor(branch_gates):\n",
    "                branch_gates = torch.tensor(\n",
    "                    branch_gates, dtype=att_weights.dtype, device=att_weights.device\n",
    "                )\n",
    "            branch_gates = branch_gates.view(1, 2).expand_as(att_weights)\n",
    "            att_weights = att_weights * branch_gates\n",
    "            att_weights = att_weights / (att_weights.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        w_img = att_weights[:, 0].view(-1, 1, 1, 1)\n",
    "        w_seq = att_weights[:, 1].view(-1, 1, 1, 1)\n",
    "        x5 = x5 * w_img\n",
    "        s = s * w_seq\n",
    "        x_s = torch.cat([x5, s], dim=1)\n",
    "        if return_stages:\n",
    "            stages[\"before_cbam\"] = x_s\n",
    "        x_s = self.cbam(x_s)\n",
    "        if return_stages:\n",
    "            stages[\"after_cbam\"] = x_s\n",
    "        x6 = self.upconv1(x_s)\n",
    "        x6 = self.bn_up1(x6)\n",
    "        x6 = torch.cat([x6, x4], dim=1)\n",
    "        x6 = self.decoder_1(x6)\n",
    "        if return_stages:\n",
    "            stages[\"dec1\"] = x6\n",
    "        x7 = self.upconv2(x6)\n",
    "        x7 = self.bn_up2(x7)\n",
    "        x7 = torch.cat([x7, x3], dim=1)\n",
    "        x7 = self.decoder_2(x7)\n",
    "        if return_stages:\n",
    "            stages[\"dec2\"] = x7\n",
    "        x8 = self.upconv3(x7)\n",
    "        x8 = self.bn_up3(x8)\n",
    "        x8 = torch.cat([x8, x2], dim=1)\n",
    "        x8 = self.decoder_3(x8)\n",
    "        if return_stages:\n",
    "            stages[\"dec3\"] = x8\n",
    "        x9 = self.upconv4(x8)\n",
    "        x9 = self.bn_up4(x9)\n",
    "        x9 = torch.cat([x9, x1], dim=1)\n",
    "        x9 = self.decoder_4(x9)\n",
    "        if return_stages:\n",
    "            stages[\"dec4\"] = x9\n",
    "        out = self.output_layer(x9)\n",
    "        out = out.squeeze(1)\n",
    "        if return_stages:\n",
    "            stages[\"output\"] = out.unsqueeze(1)\n",
    "        if not return_att and not return_stages:\n",
    "            return out\n",
    "        if return_att:\n",
    "            with torch.no_grad():\n",
    "                avg_out = torch.mean(x_s, dim=1, keepdim=True)\n",
    "                max_out, _ = torch.max(x_s, dim=1, keepdim=True)\n",
    "                spatial_input = torch.cat([avg_out, max_out], dim=1)\n",
    "                sa = self.cbam.spatial_attention(spatial_input)\n",
    "                sa_up = nn.functional.interpolate(\n",
    "                    sa, size=x_input.shape[-2:], mode='bilinear', align_corners=False\n",
    "                )\n",
    "                sa_up = sa_up.squeeze(1).detach().cpu()\n",
    "\n",
    "            att_dict = {\n",
    "                \"branch_weights\": att_weights.detach().cpu(),\n",
    "                \"spatial_map\": sa_up\n",
    "            }\n",
    "\n",
    "        if return_att and not return_stages:\n",
    "            return out, att_dict\n",
    "        elif not return_att and return_stages:\n",
    "            return out, stages\n",
    "        else:\n",
    "            return out, att_dict, stages\n",
    "\n",
    "def train_model(train_loader, val_loader, model, epochs,\n",
    "                criterion, optimizer, scheduler, save_path):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    total_train_time = 0\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        batch_loss = []\n",
    "        train_loader_tqdm = tqdm(train_loader,\n",
    "                                 desc=f\"Training Epoch {epoch + 1}/{epochs}\")\n",
    "        for img_in, seq_in, targets in train_loader_tqdm:\n",
    "            img_in = img_in.to(device)\n",
    "            seq_in = seq_in.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(img_in, seq_in)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item())\n",
    "            train_loader_tqdm.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "\n",
    "        train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "        model.eval()\n",
    "        val_batch_loss = []\n",
    "        with torch.no_grad():\n",
    "            for img_in, seq_in, targets in val_loader:\n",
    "                img_in = img_in.to(device)\n",
    "                seq_in = seq_in.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(img_in, seq_in)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_batch_loss.append(loss.item())\n",
    "\n",
    "        val_loss.append(np.mean(val_batch_loss))\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        total_train_time += epoch_time\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss[-1])\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            current_lr = None\n",
    "\n",
    "        if val_loss[-1] < best_val_loss:\n",
    "            best_val_loss = val_loss[-1]\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            lr_str = f\", LR: {current_lr:.2e}\" if current_lr is not None else \"\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            lr_str = f\", LR: {current_lr:.2e}\" if current_lr is not None else \"\"\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    avg_epoch_time = np.mean(epoch_times)\n",
    "    total_train_time_min = total_train_time / 60\n",
    "\n",
    "    return train_loss, val_loss, total_train_time_min, avg_epoch_time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
